{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "acronym-disambiguation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEkVQ33ieTHC"
      },
      "source": [
        "## Acronym Disambiguation using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeXnIUcEthYO",
        "papermill": {
          "duration": 16.6663,
          "end_time": "2020-11-10T12:34:04.018204",
          "exception": false,
          "start_time": "2020-11-10T12:33:47.351904",
          "status": "completed"
        },
        "tags": [],
        "trusted": true
      },
      "source": [
        "! pip install transformers -q\n",
        "! pip install tokenizers -q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOFhm9xStmQV",
        "papermill": {
          "duration": 1.168554,
          "end_time": "2020-11-10T12:34:05.229569",
          "exception": false,
          "start_time": "2020-11-10T12:34:04.061015",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:58:58.543286Z",
          "iopub.execute_input": "2021-10-28T13:58:58.543572Z",
          "iopub.status.idle": "2021-10-28T13:58:58.855967Z",
          "shell.execute_reply.started": "2021-10-28T13:58:58.543524Z",
          "shell.execute_reply": "2021-10-28T13:58:58.855194Z"
        },
        "trusted": true
      },
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import ast\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import matplotlib.cm as cm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import *\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.utils.extmath import softmax\n",
        "from sklearn import model_selection\n",
        "from sklearn.metrics import classification_report, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JsXpQNttsRg",
        "papermill": {
          "duration": 6.760402,
          "end_time": "2020-11-10T12:34:12.010196",
          "exception": false,
          "start_time": "2020-11-10T12:34:05.249794",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:58:58.861556Z",
          "iopub.execute_input": "2021-10-28T13:58:58.862177Z",
          "iopub.status.idle": "2021-10-28T13:59:00.756647Z",
          "shell.execute_reply.started": "2021-10-28T13:58:58.862137Z",
          "shell.execute_reply": "2021-10-28T13:59:00.755903Z"
        },
        "trusted": true
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "import tokenizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYZrbqb2d1gk",
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:00.823429Z",
          "iopub.execute_input": "2021-10-28T13:59:00.823606Z",
          "iopub.status.idle": "2021-10-28T13:59:00.828854Z",
          "shell.execute_reply.started": "2021-10-28T13:59:00.823584Z",
          "shell.execute_reply": "2021-10-28T13:59:00.828071Z"
        },
        "trusted": true
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1giNgY-Tslre"
      },
      "source": [
        "# ROOT = './drive/MyDrive/IRE Major Project/'\n",
        "ROOT = '../'\n",
        "DATAFOLDER = '../data/'\n",
        "OUTPUTFOLDER = '../outputs/'\n",
        "CONFIGFOLDER = '../models/model configs/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTdjnvlytwNV",
        "papermill": {
          "duration": 0.03079,
          "end_time": "2020-11-10T12:34:12.061159",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.030369",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:00.766307Z",
          "iopub.execute_input": "2021-10-28T13:59:00.76676Z",
          "iopub.status.idle": "2021-10-28T13:59:00.775007Z",
          "shell.execute_reply.started": "2021-10-28T13:59:00.766719Z",
          "shell.execute_reply": "2021-10-28T13:59:00.774335Z"
        },
        "trusted": true
      },
      "source": [
        "def seed_all(seed = 42):\n",
        "  \"\"\"\n",
        "  Fix seed for reproducibility\n",
        "  \"\"\"\n",
        "  # python RNG\n",
        "  import random\n",
        "  random.seed(seed)\n",
        "\n",
        "  # pytorch RNGs\n",
        "  import torch\n",
        "  torch.manual_seed(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "  # numpy RNG\n",
        "  import numpy as np\n",
        "  np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mp3ZprESaL4H"
      },
      "source": [
        "## Data augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f3IXQL8aUv6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqrGLZiVaZvv"
      },
      "source": [
        "! pip install python-Levenshtein"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K080JMEVaOl4"
      },
      "source": [
        "# LOAD DATA\n",
        "with open(\"diction.json\") as f:\n",
        "  input_dict = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXYletekbdOK"
      },
      "source": [
        "## Levenshtein Distance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmOw1ZeDaxRR"
      },
      "source": [
        "from Levenshtein import distance as levenshtein_distance\n",
        "\n",
        "output_dict = {}\n",
        "for key,val in input_dict.items():\n",
        "  dataset = set(val)\n",
        "  new_dat = list(dataset)\n",
        "  # Lowercase\n",
        "  new_dat = [a.lower() for a in new_dat]\n",
        "  new_dat.sort()\n",
        "  # Remove punctuations and spaces\n",
        "  clean_acronyms = [s.translate(str.maketrans(' ', ' ', string.punctuation)) for s in new_dat]\n",
        "  new_dat = [s.translate(str.maketrans('', '', string.punctuation)).replace(\" \",\"\") for s in new_dat]\n",
        "\n",
        "  # Generate levenshtein dist matrix for each pair\n",
        "  n = len(new_dat)\n",
        "  mat = np.zeros(shape=(n,n))\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      mat[i][j] = (levenshtein_distance(new_dat[i], new_dat[j]))\n",
        "\n",
        "  # Heatmap of matrix\n",
        "  plt.imshow(mat, cmap='hot', interpolation='nearest')\n",
        "  plt.colorbar()\n",
        "  plt.show()\n",
        "\n",
        "  THRESHOLD = 0.4\n",
        "  acdict = {}\n",
        "  visited = np.zeros(shape=(n,1))\n",
        "\n",
        "  for id,val in enumerate(new_dat):\n",
        "    if not visited[id]:\n",
        "      acdict[clean_acronyms[id]] = {}\n",
        "      for idx, a in enumerate(mat[id]):\n",
        "        # Cluster the expansion according to threshold value\n",
        "        if not visited[idx] and float(a/len(val)) <= THRESHOLD:\n",
        "          visited[idx] = 1\n",
        "          if val in acdict[clean_acronyms[id]]:\n",
        "            acdict[clean_acronyms[id]][val].append(clean_acronyms[idx])\n",
        "            continue\n",
        "          acdict[clean_acronyms[id]][val] = [clean_acronyms[idx]]\n",
        "      visited[id] = 1\n",
        "  output_dict[key] = acdict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUqmJZ8xazck"
      },
      "source": [
        "import pprint\n",
        "pp = pprint.PrettyPrinter(indent = 3)\n",
        "pp.pprint(output_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHGCkwuabWYL"
      },
      "source": [
        "import json\n",
        "with open(DATAFOLDER + \"scientific/combined_acronym_dict_sci.json\",\"w\") as f:\n",
        "  json.dump(output_dict,f,indent=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owj2UQx9cAAp"
      },
      "source": [
        "### Update dataset with new expansions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTvw6VhwcEgQ"
      },
      "source": [
        "with open(DATAFOLDER + \"scientific/fix_combined_acronym_dict_sci.json\") as f:\n",
        "  data = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68ZdTRSOcVV5"
      },
      "source": [
        "STORE_FILE = DATAFOLDER + \"scientific/train.csv\"\n",
        "# train_df = pd.read_csv(\"train.csv\")\n",
        "df = pd.read_csv(STORE_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uk2OuYIocYh_"
      },
      "source": [
        "def func(ser, isseries = False):\n",
        "  if isseries:\n",
        "    acr = ser[0]\n",
        "    val = ser[1]\n",
        "    val = val.lower()\n",
        "    val = val.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    if acr not in data:\n",
        "      return val\n",
        "    for key,v in data[acr].items():\n",
        "      for k, va in v.items():\n",
        "        for vals in va:\n",
        "          if vals == val:\n",
        "            return key\n",
        "    return val\n",
        "  else:\n",
        "    acr = ser[0]\n",
        "    val = ser[1]\n",
        "    val = val.lower()\n",
        "    val = val.translate(str.maketrans(' ', ' ', string.punctuation))\n",
        "    if acr not in data:\n",
        "      return val\n",
        "    for key,v in data[acr].items():\n",
        "      for k, va in v.items():\n",
        "        for vals in va:\n",
        "          if vals == val:\n",
        "            return key\n",
        "    return val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PW80Af8cemh"
      },
      "source": [
        "# Update expansions in data\n",
        "df['expansion'] = df.apply(lambda x:func(x, True),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPQCCrhzcjdD"
      },
      "source": [
        "df.to_csv(\"update_\"+STORE_FILE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W7p01CXcoj-"
      },
      "source": [
        "with open(DATAFOLDER + \"scientific/diction.json\") as f:\n",
        "  diction = json.load(f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2R47gwBncqix"
      },
      "source": [
        "# Update diction\n",
        "for key, val in diction.items():\n",
        "  for idx, acr in enumerate(val):\n",
        "    val[idx] = func([key,acr], True)\n",
        "\n",
        "with open(DATAFOLDER + \"scientific/diction_update.json\",\"w\") as f:\n",
        "  json.dump(diction,f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWt01JPpea4l"
      },
      "source": [
        "### Model Configurations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMHkNSVGumlX",
        "papermill": {
          "duration": 0.071024,
          "end_time": "2020-11-10T12:34:12.154704",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.08368",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:00.776236Z",
          "iopub.execute_input": "2021-10-28T13:59:00.776554Z",
          "iopub.status.idle": "2021-10-28T13:59:00.820736Z",
          "shell.execute_reply.started": "2021-10-28T13:59:00.776515Z",
          "shell.execute_reply": "2021-10-28T13:59:00.819981Z"
        },
        "trusted": true
      },
      "source": [
        "class config:\n",
        "  SEED = 42\n",
        "  KFOLD = 5\n",
        "  TRAIN_FILE = DATAFOLDER + 'scientific/update_train.csv'\n",
        "  VAL_FILE = DATAFOLDER + 'scientific/update_dev.csv'\n",
        "  TEST_FILE = DATAFOLDER + 'scientific/test.csv'\n",
        "  SAVE_DIR = OUTPUTFOLDER\n",
        "  MAX_LEN = 192\n",
        "  MODEL = 'allenai/scibert_scivocab_uncased'\n",
        "  CONFIG = CONFIGFOLDER + 'finetune_scibert_config.json'\n",
        "  TOKENIZER = tokenizers.BertWordPieceTokenizer(CONFIGFOLDER+\"finetune_scibert_vocab.txt\", lowercase=True)\n",
        "  EPOCHS = 10\n",
        "  TRAIN_BATCH_SIZE = 32\n",
        "  VALID_BATCH_SIZE = 32\n",
        "  TEST_BATCH_SIZE = 32\n",
        "  DICTIONARY = json.load(open(DATAFOLDER + 'scientific/diction_update.json'))\n",
        "  \n",
        "  A2ID = {}\n",
        "  for idx,(k, v) in enumerate(DICTIONARY.items()):\n",
        "    for w in v:\n",
        "      A2ID[w] = len(A2ID)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-_-jJZ9zYYN",
        "papermill": {
          "duration": 0.030981,
          "end_time": "2020-11-10T12:34:12.205672",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.174691",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:00.830249Z",
          "iopub.execute_input": "2021-10-28T13:59:00.830961Z",
          "iopub.status.idle": "2021-10-28T13:59:00.837726Z",
          "shell.execute_reply.started": "2021-10-28T13:59:00.830923Z",
          "shell.execute_reply": "2021-10-28T13:59:00.837017Z"
        },
        "trusted": true
      },
      "source": [
        "class AverageMeter:\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9uPTGIo1ihU",
        "papermill": {
          "duration": 0.038092,
          "end_time": "2020-11-10T12:34:12.263814",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.225722",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:03.272467Z",
          "iopub.execute_input": "2021-10-28T13:59:03.273024Z",
          "iopub.status.idle": "2021-10-28T13:59:03.28655Z",
          "shell.execute_reply.started": "2021-10-28T13:59:03.272983Z",
          "shell.execute_reply": "2021-10-28T13:59:03.285722Z"
        },
        "trusted": true
      },
      "source": [
        "class EarlyStopping:\n",
        "    \n",
        "    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.counter = 0\n",
        "        self.mode = mode\n",
        "        self.best_score = None\n",
        "        self.output = None\n",
        "        self.early_stop = False\n",
        "        self.delta = delta\n",
        "        if self.mode == \"min\":\n",
        "            self.val_score = np.Inf\n",
        "        else:\n",
        "            self.val_score = -np.Inf\n",
        "\n",
        "    def __call__(self, epoch_score, res, model, model_path):\n",
        "        if self.mode == \"min\":\n",
        "            score = -1.0 * epoch_score\n",
        "        else:\n",
        "            score = np.copy(epoch_score)\n",
        "        if self.best_score is None:\n",
        "            self.best_score = score\n",
        "            self.output = res\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "        elif score < self.best_score + self.delta:\n",
        "            self.counter += 1\n",
        "            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = score\n",
        "            self.output = res\n",
        "            self.save_checkpoint(epoch_score, model, model_path)\n",
        "            self.counter = 0\n",
        "\n",
        "    def save_checkpoint(self, epoch_score, model, model_path):\n",
        "        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n",
        "            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        self.val_score = epoch_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssCvflkBANC5",
        "papermill": {
          "duration": 0.029814,
          "end_time": "2020-11-10T12:34:12.313913",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.284099",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:03.436164Z",
          "iopub.execute_input": "2021-10-28T13:59:03.436781Z",
          "iopub.status.idle": "2021-10-28T13:59:03.442975Z",
          "shell.execute_reply.started": "2021-10-28T13:59:03.436739Z",
          "shell.execute_reply": "2021-10-28T13:59:03.442147Z"
        },
        "trusted": true
      },
      "source": [
        "def sample_text(text, acronym, max_len):\n",
        "  text = text.split()\n",
        "  try:\n",
        "    idx = text.index(acronym)\n",
        "  except:\n",
        "    idx = [i for i, s in enumerate(text) if acronym in s]\n",
        "    idx = idx[0]\n",
        "  left_idx = max(0, idx - max_len//2)\n",
        "  right_idx = min(len(text), idx + max_len//2)\n",
        "  sampled_text = text[left_idx:right_idx]\n",
        "  return ' '.join(sampled_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoZ1L2smuMDd",
        "papermill": {
          "duration": 0.048174,
          "end_time": "2020-11-10T12:34:12.382451",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.334277",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:03.561903Z",
          "iopub.execute_input": "2021-10-28T13:59:03.56222Z",
          "iopub.status.idle": "2021-10-28T13:59:03.579561Z",
          "shell.execute_reply.started": "2021-10-28T13:59:03.562191Z",
          "shell.execute_reply": "2021-10-28T13:59:03.578812Z"
        },
        "trusted": true
      },
      "source": [
        "def process_data(text, acronym, expansion, tokenizer, max_len):\n",
        "\n",
        "  text = str(text)\n",
        "  expansion = str(expansion)\n",
        "  acronym = str(acronym)\n",
        "\n",
        "  n_tokens = len(text.split())\n",
        "  if n_tokens>120:\n",
        "    text = sample_text(text, acronym, 120)\n",
        "\n",
        "  answers = acronym + ' ' + ' '.join(config.DICTIONARY[acronym])\n",
        "  start = answers.find(expansion)\n",
        "  end = start + len(expansion)\n",
        "\n",
        "  char_mask = [0]*len(answers)\n",
        "  for i in range(start, end):\n",
        "    char_mask[i] = 1\n",
        "  \n",
        "  tok_answer = tokenizer.encode(answers)\n",
        "  answer_ids = tok_answer.ids\n",
        "  answer_offsets = tok_answer.offsets\n",
        "\n",
        "  answer_ids = answer_ids[1:-1]\n",
        "  answer_offsets = answer_offsets[1:-1]\n",
        "\n",
        "  target_idx = []\n",
        "  for i, (off1, off2) in enumerate(answer_offsets):\n",
        "      if sum(char_mask[off1:off2])>0:\n",
        "        target_idx.append(i)\n",
        "\n",
        "  start = target_idx[0]\n",
        "  end = target_idx[-1]\n",
        "\n",
        "  \n",
        "  text_ids = tokenizer.encode(text).ids[1:-1]\n",
        "\n",
        "  token_ids = [101] + answer_ids + [102] + text_ids + [102]\n",
        "  offsets =   [(0,0)] + answer_offsets + [(0,0)]*(len(text_ids) + 2)\n",
        "  mask = [1] * len(token_ids)\n",
        "  token_type = [0]*(len(answer_ids) + 1) + [1]*(2+len(text_ids))\n",
        "\n",
        "  text = answers + text\n",
        "  start = start + 1\n",
        "  end = end + 1\n",
        "\n",
        "  padding = max_len - len(token_ids)\n",
        "    \n",
        "\n",
        "  if padding>=0:\n",
        "    token_ids = token_ids + ([0] * padding)\n",
        "    token_type = token_type + [1] * padding\n",
        "    mask = mask + ([0] * padding)\n",
        "    offsets = offsets + ([(0, 0)] * padding)\n",
        "  else:\n",
        "    token_ids = token_ids[0:max_len]\n",
        "    token_type = token_type[0:max_len]\n",
        "    mask = mask[0:max_len]\n",
        "    offsets = offsets[0:max_len]\n",
        "  \n",
        "\n",
        "  assert len(token_ids)==max_len\n",
        "  assert len(mask)==max_len\n",
        "  assert len(offsets)==max_len\n",
        "  assert len(token_type)==max_len\n",
        "\n",
        "  return {\n",
        "          'ids': token_ids,\n",
        "          'mask': mask,\n",
        "          'token_type': token_type,\n",
        "          'offset': offsets,\n",
        "          'start': start,\n",
        "          'end': end,  \n",
        "          'text': text,\n",
        "          'expansion': expansion,\n",
        "          'acronym': acronym,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lVZx3kZ3euOz"
      },
      "source": [
        "### Dataset Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUhLupPgwL1M",
        "papermill": {
          "duration": 0.036999,
          "end_time": "2020-11-10T12:34:12.440318",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.403319",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:03.704641Z",
          "iopub.execute_input": "2021-10-28T13:59:03.705051Z",
          "iopub.status.idle": "2021-10-28T13:59:03.7138Z",
          "shell.execute_reply.started": "2021-10-28T13:59:03.705018Z",
          "shell.execute_reply": "2021-10-28T13:59:03.713116Z"
        },
        "trusted": true
      },
      "source": [
        "class Dataset:\n",
        "    def __init__(self, text, acronym, expansion):\n",
        "        self.text = text\n",
        "        self.acronym = acronym\n",
        "        self.expansion = expansion\n",
        "        self.tokenizer = config.TOKENIZER\n",
        "        self.max_len = config.MAX_LEN\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        data = process_data(\n",
        "            self.text[item],\n",
        "            self.acronym[item],\n",
        "            self.expansion[item], \n",
        "            self.tokenizer,\n",
        "            self.max_len,\n",
        "            \n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(data['ids'], dtype=torch.long),\n",
        "            'mask': torch.tensor(data['mask'], dtype=torch.long),\n",
        "            'token_type': torch.tensor(data['token_type'], dtype=torch.long),\n",
        "            'offset': torch.tensor(data['offset'], dtype=torch.long),\n",
        "            'start': torch.tensor(data['start'], dtype=torch.long),\n",
        "            'end': torch.tensor(data['end'], dtype=torch.long),\n",
        "            'text': data['text'],\n",
        "            'expansion': data['expansion'],\n",
        "            'acronym': data['acronym'],\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSLoNTjXKfcj",
        "papermill": {
          "duration": 0.029073,
          "end_time": "2020-11-10T12:34:12.490614",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.461541",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:03.843825Z",
          "iopub.execute_input": "2021-10-28T13:59:03.844146Z",
          "iopub.status.idle": "2021-10-28T13:59:03.848512Z",
          "shell.execute_reply.started": "2021-10-28T13:59:03.844119Z",
          "shell.execute_reply": "2021-10-28T13:59:03.847597Z"
        },
        "trusted": true
      },
      "source": [
        "def get_loss(start, start_logits, end, end_logits):\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  start_loss = loss_fn(start_logits, start)\n",
        "  end_loss = loss_fn(end_logits, end)\n",
        "  loss = start_loss + end_loss\n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sk83SAi9ew29"
      },
      "source": [
        "### BERT class \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J4LXtTDEOWt",
        "papermill": {
          "duration": 0.033596,
          "end_time": "2020-11-10T12:34:12.545379",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.511783",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:04.138575Z",
          "iopub.execute_input": "2021-10-28T13:59:04.138994Z",
          "iopub.status.idle": "2021-10-28T13:59:04.148439Z",
          "shell.execute_reply.started": "2021-10-28T13:59:04.138958Z",
          "shell.execute_reply": "2021-10-28T13:59:04.145677Z"
        },
        "trusted": true
      },
      "source": [
        "class BertAD(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(BertAD, self).__init__()\n",
        "    self.model_config = transformers.BertConfig.from_pretrained(config.MODEL)\n",
        "    self.bert = transformers.BertModel.from_pretrained(config.MODEL, config=config.CONFIG)\n",
        "    self.layer = nn.Linear(768, 2)\n",
        "    \n",
        "\n",
        "  def forward(self, ids, mask, token_type, start=None, end=None):\n",
        "    output = self.bert(input_ids = ids,\n",
        "                       attention_mask = mask,\n",
        "                       token_type_ids = token_type)\n",
        "    \n",
        "    logits = self.layer(output[0]) \n",
        "    start_logits, end_logits = logits.split(1, dim=-1)\n",
        "    \n",
        "    start_logits = start_logits.squeeze(-1)\n",
        "    end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "    loss = get_loss(start, start_logits, end, end_logits)    \n",
        "\n",
        "    return loss, start_logits, end_logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NGJrNcSx33W",
        "papermill": {
          "duration": 0.03494,
          "end_time": "2020-11-10T12:34:12.600929",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.565989",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:04.411984Z",
          "iopub.execute_input": "2021-10-28T13:59:04.412604Z",
          "iopub.status.idle": "2021-10-28T13:59:04.420732Z",
          "shell.execute_reply.started": "2021-10-28T13:59:04.412568Z",
          "shell.execute_reply": "2021-10-28T13:59:04.419926Z"
        },
        "trusted": true
      },
      "source": [
        "def train_fn(data_loader, model, optimizer, device):\n",
        "  model.train()\n",
        "  losses = AverageMeter()\n",
        "  tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "  \n",
        "  for bi, d in enumerate(tk0):\n",
        "    ids = d['ids']\n",
        "    mask = d['mask']\n",
        "    token_type = d['token_type']\n",
        "    start = d['start']\n",
        "    end = d['end']\n",
        "    \n",
        "\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    token_type = token_type.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    start = start.to(device, dtype=torch.long)\n",
        "    end = end.to(device, dtype=torch.long)\n",
        "    \n",
        "\n",
        "    model.zero_grad()\n",
        "    loss, start_logits, end_logits = model(ids, mask, token_type, start, end)\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    losses.update(loss.item(), ids.size(0))\n",
        "    tk0.set_postfix(loss=losses.avg)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzctCj1Rx-qD",
        "papermill": {
          "duration": 0.02935,
          "end_time": "2020-11-10T12:34:12.651711",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.622361",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:04.537614Z",
          "iopub.execute_input": "2021-10-28T13:59:04.537922Z",
          "iopub.status.idle": "2021-10-28T13:59:04.542757Z",
          "shell.execute_reply.started": "2021-10-28T13:59:04.537892Z",
          "shell.execute_reply": "2021-10-28T13:59:04.541871Z"
        },
        "trusted": true
      },
      "source": [
        "def jaccard(str1, str2): \n",
        "    a = set(str1.lower().split()) \n",
        "    b = set(str2.lower().split())\n",
        "    c = a.intersection(b)\n",
        "    #     if  len(a) + len(b) == len(c):\n",
        "    #          print(f'{str1},{str2}')\n",
        "    #          return 1.0\n",
        "    return float(len(c)) / (len(a) + len(b) - len(c))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaeQFvcTxB6p",
        "papermill": {
          "duration": 0.032932,
          "end_time": "2020-11-10T12:34:12.705632",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.6727",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:04.694252Z",
          "iopub.execute_input": "2021-10-28T13:59:04.694547Z",
          "iopub.status.idle": "2021-10-28T13:59:04.701022Z",
          "shell.execute_reply.started": "2021-10-28T13:59:04.694519Z",
          "shell.execute_reply": "2021-10-28T13:59:04.699994Z"
        },
        "trusted": true
      },
      "source": [
        "def evaluate_jaccard(text, selected_text, acronym, offsets, idx_start, idx_end):\n",
        "  filtered_output = \"\"\n",
        "  for ix in range(idx_start, idx_end + 1):\n",
        "      filtered_output += text[offsets[ix][0]: offsets[ix][1]]\n",
        "      if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n",
        "          filtered_output += \" \"\n",
        "\n",
        "  candidates = config.DICTIONARY[acronym]\n",
        "  candidate_jaccards = [jaccard(w.strip(), filtered_output.strip()) for w in candidates]\n",
        "  idx = np.argmax(candidate_jaccards)\n",
        "\n",
        "  return candidate_jaccards[idx], candidates[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSQNnIl-x3om",
        "papermill": {
          "duration": 0.046904,
          "end_time": "2020-11-10T12:34:12.773817",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.726913",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:04.931839Z",
          "iopub.execute_input": "2021-10-28T13:59:04.932068Z",
          "iopub.status.idle": "2021-10-28T13:59:04.947914Z",
          "shell.execute_reply.started": "2021-10-28T13:59:04.932043Z",
          "shell.execute_reply": "2021-10-28T13:59:04.947246Z"
        },
        "trusted": true
      },
      "source": [
        "def eval_fn(data_loader, model, device):\n",
        "  model.eval()\n",
        "  losses = AverageMeter()\n",
        "  jac = AverageMeter()\n",
        "\n",
        "  tk0 = tqdm(data_loader, total=len(data_loader))\n",
        "\n",
        "  pred_expansion_ = []\n",
        "  true_expansion_ = []\n",
        "  acronym_ = []\n",
        "  text_ = []\n",
        "\n",
        "  for bi, d in enumerate(tk0):\n",
        "    ids = d['ids']\n",
        "    mask = d['mask']\n",
        "    token_type = d['token_type']\n",
        "    start = d['start']\n",
        "    end = d['end']\n",
        "    \n",
        "    text = d['text']\n",
        "    expansion = d['expansion']\n",
        "    offset = d['offset']\n",
        "    acronym = d['acronym']\n",
        "\n",
        "\n",
        "    ids = ids.to(device, dtype=torch.long)\n",
        "    mask = mask.to(device, dtype=torch.long)\n",
        "    token_type = token_type.to(device, dtype=torch.long)\n",
        "    start = start.to(device, dtype=torch.long)\n",
        "    end = end.to(device, dtype=torch.long)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      loss, start_logits, end_logits = model(ids, mask, token_type, start, end)\n",
        "\n",
        "\n",
        "    start_prob = torch.softmax(start_logits, dim=1).detach().cpu().numpy()\n",
        "    end_prob = torch.softmax(end_logits, dim=1).detach().cpu().numpy()\n",
        "  \n",
        "  \n",
        "    jac_= []\n",
        "    for px, s in enumerate(text):\n",
        "      start_idx = np.argmax(start_prob[px,:])\n",
        "      end_idx = np.argmax(end_prob[px,:])\n",
        "\n",
        "      js, exp = evaluate_jaccard(s, expansion[px], acronym[px], offset[px], start_idx, end_idx)\n",
        "      jac_.append(js)\n",
        "#       print(f'acronym:{acronym[px]},s:{s},exp:{exp},expansion[px]:{expansion[px]}')\n",
        "      pred_expansion_.append(exp)\n",
        "      true_expansion_.append(expansion[px])\n",
        "      text_.append(s)\n",
        "      acronym_.append(acronym[px])\n",
        "        \n",
        "\n",
        "    \n",
        "    jac.update(np.mean(jac_), len(jac_))\n",
        "    losses.update(loss.item(), ids.size(0))\n",
        "\n",
        "    tk0.set_postfix(loss=losses.avg, jaccard=jac.avg)\n",
        "\n",
        "\n",
        "  pred_expansion_1 = [config.A2ID[w] for w in pred_expansion_]\n",
        "  true_expansion_1 = [config.A2ID[w] for w in true_expansion_]\n",
        "  \n",
        "  f1 = f1_score(true_expansion_1, pred_expansion_1, average='macro')\n",
        "\n",
        "  print('Average Jaccard : ', jac.avg)\n",
        "  print('Macro F1 : ', f1)\n",
        "  dit = {'acronym':acronym_,'actual':true_expansion_,'prediction':pred_expansion_, 'text':text_}\n",
        "  res = pd.DataFrame(dit)\n",
        "  return f1, res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BL1F0LV_RNNk",
        "papermill": {
          "duration": 0.042771,
          "end_time": "2020-11-10T12:34:12.838755",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.795984",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:05.322378Z",
          "iopub.execute_input": "2021-10-28T13:59:05.322997Z",
          "iopub.status.idle": "2021-10-28T13:59:05.336966Z",
          "shell.execute_reply.started": "2021-10-28T13:59:05.322959Z",
          "shell.execute_reply": "2021-10-28T13:59:05.33502Z"
        },
        "trusted": true
      },
      "source": [
        "def run(df_train, df_val, fold):\n",
        "  train_dataset = Dataset(\n",
        "        text = df_train.text.values,\n",
        "        acronym = df_train.acronym_.values,\n",
        "        expansion = df_train.expansion.values\n",
        "    )\n",
        "  \n",
        "  valid_dataset = Dataset(\n",
        "        text = df_val.text.values,\n",
        "        acronym = df_val.acronym_.values,\n",
        "        expansion = df_val.expansion.values,\n",
        "    )\n",
        "    \n",
        "  train_data_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.TRAIN_BATCH_SIZE,\n",
        "        num_workers=2\n",
        "    )\n",
        "\n",
        "  valid_data_loader = torch.utils.data.DataLoader(\n",
        "        valid_dataset,\n",
        "        batch_size=config.VALID_BATCH_SIZE,\n",
        "        num_workers=2\n",
        "    )\n",
        "  test_dataset = Dataset(\n",
        "        text = df_test.text.values,\n",
        "        acronym = df_test.acronym_.values,\n",
        "        expansion = None\n",
        "    )\n",
        "    \n",
        "  test_data_loader = torch.utils.data.DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=config.TEST_BATCH_SIZE,\n",
        "        num_workers=2\n",
        "    )\n",
        "  \n",
        "\n",
        "  model = BertAD()\n",
        "  device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
        "  model.to(device)\n",
        "\n",
        "  lr = 2e-5\n",
        "  param_optimizer = list(model.named_parameters())\n",
        "  no_decay = ['bias', 'gamma', 'beta']\n",
        "  optimizer_grouped_parameters = [\n",
        "      {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.01},\n",
        "      {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "      'weight_decay_rate': 0.0}\n",
        "  ]\n",
        "  optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n",
        "\n",
        "  es = EarlyStopping(patience=2, mode=\"max\")\n",
        "\n",
        "  print('Starting training....')\n",
        "  for epoch in range(config.EPOCHS):\n",
        "    train_fn(train_data_loader, model, optimizer, device)\n",
        "    valid_loss, res = eval_fn(valid_data_loader, model, device)\n",
        "    print(f'Fold {fold} | Epoch :{epoch + 1} | Validation Score :{valid_loss}')\n",
        "    if fold is None:\n",
        "      es(valid_loss, res, model, model_path=os.path.join(config.SAVE_DIR, \"model.bin\"))\n",
        "    else:\n",
        "      es(valid_loss, res, model, model_path=os.path.join(config.SAVE_DIR, f\"model_{fold}.bin\"))\n",
        "    if es.early_stop:\n",
        "      break\n",
        "\n",
        "  return es.best_score, es.output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cFwO08gRrMw",
        "papermill": {
          "duration": 0.033335,
          "end_time": "2020-11-10T12:34:12.894703",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.861368",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:05.497185Z",
          "iopub.execute_input": "2021-10-28T13:59:05.497411Z",
          "iopub.status.idle": "2021-10-28T13:59:05.504624Z",
          "shell.execute_reply.started": "2021-10-28T13:59:05.497385Z",
          "shell.execute_reply": "2021-10-28T13:59:05.50387Z"
        },
        "trusted": true
      },
      "source": [
        "def run_k_fold(fold_id):\n",
        "  '''\n",
        "    Perform k-fold cross-validation\n",
        "  '''\n",
        "  seed_all()\n",
        "\n",
        "  df_train = pd.read_csv(config.TRAIN_FILE)\n",
        "  df_val = pd.read_csv(config.VAL_FILE)\n",
        "  df_test = pd.read_csv(config.TEST_FILE)\n",
        "  df_train.fillna('NA',inplace=True)\n",
        "  # concatenating train and validation set\n",
        "  train = pd.concat([df_train, df_val]).reset_index()\n",
        "  \n",
        "  # dividing folds\n",
        "  kf = model_selection.StratifiedKFold(n_splits=config.KFOLD, shuffle=True, random_state=config.SEED)\n",
        "  for fold, (train_idx, val_idx) in enumerate(kf.split(X=train, y=train.acronym_.values)):\n",
        "      train.loc[val_idx, 'kfold'] = fold\n",
        "\n",
        "  print(f'################################################ Fold {fold_id} #################################################')\n",
        "  df_train = train[train.kfold!=fold_id]\n",
        "  df_val = train[train.kfold==fold_id]\n",
        "\n",
        "  return run(df_train, df_val, fold_id)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEBaP3xRe-97"
      },
      "source": [
        "### Train and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XouQZpbQHxoC",
        "papermill": {
          "duration": 4379.503848,
          "end_time": "2020-11-10T13:47:12.419425",
          "exception": false,
          "start_time": "2020-11-10T12:34:12.915577",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2021-10-28T13:59:05.672427Z",
          "iopub.execute_input": "2021-10-28T13:59:05.67276Z",
          "iopub.status.idle": "2021-10-28T15:27:58.509632Z",
          "shell.execute_reply.started": "2021-10-28T13:59:05.672728Z",
          "shell.execute_reply": "2021-10-28T15:27:58.508625Z"
        },
        "trusted": true
      },
      "source": [
        "f0, res0 = run_k_fold(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 4352.272452,
          "end_time": "2020-11-10T14:59:44.724478",
          "exception": false,
          "start_time": "2020-11-10T13:47:12.452026",
          "status": "completed"
        },
        "tags": [],
        "id": "2iqeRa_fdRHv",
        "trusted": true
      },
      "source": [
        "f1, res1 = run_k_fold(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 4357.22236,
          "end_time": "2020-11-10T16:12:21.989708",
          "exception": false,
          "start_time": "2020-11-10T14:59:44.767348",
          "status": "completed"
        },
        "tags": [],
        "id": "Ldi1ht16dRHv",
        "trusted": true
      },
      "source": [
        "f2, res2 = run_k_fold(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 4354.081987,
          "end_time": "2020-11-10T17:24:56.123577",
          "exception": false,
          "start_time": "2020-11-10T16:12:22.04159",
          "status": "completed"
        },
        "tags": [],
        "id": "IaBib92IdRHw",
        "trusted": true
      },
      "source": [
        "f3, res3 = run_k_fold(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 4349.047292,
          "end_time": "2020-11-10T18:37:25.259088",
          "exception": false,
          "start_time": "2020-11-10T17:24:56.211796",
          "status": "completed"
        },
        "tags": [],
        "id": "2bKJgN-xdRHw",
        "trusted": true
      },
      "source": [
        "f4, res4 = run_k_fold(4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "papermill": {
          "duration": 0.078542,
          "end_time": "2020-11-10T18:37:25.406231",
          "exception": false,
          "start_time": "2020-11-10T18:37:25.327689",
          "status": "completed"
        },
        "tags": [],
        "id": "5hOvG__AdRHx",
        "trusted": true
      },
      "source": [
        "f = [f0, f1, f2, f3, f4]\n",
        "for i, fs in enumerate(f):\n",
        "    print(f'Fold {i} : {fs}')\n",
        "print(f'Avg. {np.mean(f)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-10-28T15:27:58.513117Z",
          "iopub.execute_input": "2021-10-28T15:27:58.513379Z",
          "iopub.status.idle": "2021-10-28T15:27:58.669329Z",
          "shell.execute_reply.started": "2021-10-28T15:27:58.513344Z",
          "shell.execute_reply": "2021-10-28T15:27:58.668562Z"
        },
        "trusted": true,
        "id": "6GJJZaBma2P6"
      },
      "source": [
        "print(res0)\n",
        "res0.to_csv(OUTPUTFOLDER+'scientific/output_sci.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPx6TanydRZx"
      },
      "source": [
        "## Test data prediction\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY2wEFVrdThO"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model0 = BertAD()\n",
        "vec = model0.state_dict()['bert.embeddings.position_ids']\n",
        "chkp = torch.load(os.path.join(config.SAVE_DIR, '../input/modeltest/model_0.bin'), map_location=device)\n",
        "chkp['bert.embeddings.position_ids'] = vec\n",
        "model0.load_state_dict(chkp)\n",
        "model0.to(device)\n",
        "model0.eval()\n",
        "\n",
        "model1 = BertAD()\n",
        "chkp = torch.load(os.path.join(config.SAVE_DIR, 'model_1.bin'), map_location=device)\n",
        "chkp['bert.embeddings.position_ids'] = vec\n",
        "model1.load_state_dict(chkp)\n",
        "model1.to(device)\n",
        "model1.eval()\n",
        "\n",
        "\n",
        "model2 = BertAD()\n",
        "chkp = torch.load(os.path.join(config.SAVE_DIR, 'model_2.bin'), map_location=device)\n",
        "chkp['bert.embeddings.position_ids'] = vec\n",
        "model2.load_state_dict(chkp)\n",
        "model2.to(device)\n",
        "model2.eval()\n",
        "\n",
        "model3 = BertAD()\n",
        "chkp = torch.load(os.path.join(config.SAVE_DIR, 'model_3.bin'), map_location=device)\n",
        "chkp['bert.embeddings.position_ids'] = vec\n",
        "model3.load_state_dict(chkp)\n",
        "model3.to(device)\n",
        "model3.eval()\n",
        "\n",
        "model4 = BertAD()\n",
        "chkp = torch.load(os.path.join(config.SAVE_DIR, 'model_4.bin'), map_location=device)\n",
        "chkp['bert.embeddings.position_ids'] = vec\n",
        "model4.load_state_dict(chkp)\n",
        "model4.to(device)\n",
        "model4.eval()\n",
        "print('Models loaded')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvIyJLb2dW65"
      },
      "source": [
        "test = pd.read_csv(config.TEST_FILE)\n",
        "test['expansion'] = test['acronym_']\n",
        "\n",
        "test_dataset = Dataset(\n",
        "        text = test.text.values,\n",
        "        acronym = test.acronym_.values,\n",
        "        expansion = test.expansion.values,\n",
        "    )\n",
        "    \n",
        "  \n",
        "test_data_loader = torch.utils.data.DataLoader(\n",
        "      test_dataset,\n",
        "      batch_size=config.VALID_BATCH_SIZE,\n",
        "      num_workers=2\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ucbhW3ldaNo"
      },
      "source": [
        "jac = AverageMeter()\n",
        "\n",
        "tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n",
        "\n",
        "pred_expansion_ = []\n",
        "true_expansion_ = []\n",
        "\n",
        "for bi, d in enumerate(tk0):\n",
        "  ids = d['ids']\n",
        "  mask = d['mask']\n",
        "  token_type = d['token_type']\n",
        "  start = d['start']\n",
        "  end = d['end']\n",
        "  \n",
        "  text = d['text']\n",
        "  expansion = d['expansion']\n",
        "  offset = d['offset']\n",
        "  acronym = d['acronym']\n",
        "\n",
        "\n",
        "  ids = ids.to(device, dtype=torch.long)\n",
        "  mask = mask.to(device, dtype=torch.long)\n",
        "  token_type = token_type.to(device, dtype=torch.long)\n",
        "  start = start.to(device, dtype=torch.long)\n",
        "  end = end.to(device, dtype=torch.long)\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    _, start_logits_0, end_logits_0 = model0(ids, mask, token_type, start, end)\n",
        "    _, start_logits_1, end_logits_1 = model1(ids, mask, token_type, start, end)\n",
        "    _, start_logits_2, end_logits_2 = model2(ids, mask, token_type, start, end)\n",
        "    _, start_logits_3, end_logits_3 = model3(ids, mask, token_type, start, end)\n",
        "    _, start_logits_4, end_logits_4 = model4(ids, mask, token_type, start, end)\n",
        "\n",
        "    \n",
        "  start_logits_0 = torch.softmax(start_logits_0, dim=1).detach().cpu().numpy()\n",
        "  start_logits_1 = torch.softmax(start_logits_1, dim=1).detach().cpu().numpy()\n",
        "  start_logits_2 = torch.softmax(start_logits_2, dim=1).detach().cpu().numpy()\n",
        "  start_logits_3 = torch.softmax(start_logits_3, dim=1).detach().cpu().numpy()\n",
        "  start_logits_4 = torch.softmax(start_logits_4, dim=1).detach().cpu().numpy()\n",
        "  \n",
        "    \n",
        "  end_logits_0 = torch.softmax(end_logits_0, dim=1).detach().cpu().numpy()\n",
        "  end_logits_1 = torch.softmax(end_logits_1, dim=1).detach().cpu().numpy()\n",
        "  end_logits_2 = torch.softmax(end_logits_2, dim=1).detach().cpu().numpy()\n",
        "  end_logits_3 = torch.softmax(end_logits_3, dim=1).detach().cpu().numpy()\n",
        "  end_logits_4 = torch.softmax(end_logits_4, dim=1).detach().cpu().numpy()\n",
        "  \n",
        "\n",
        "  start_prob = (start_logits_0 + start_logits_1 + start_logits_2 + start_logits_3 + start_logits_4)/5.0\n",
        "  end_prob = (end_logits_0 + end_logits_1 + end_logits_2 + end_logits_3 + end_logits_4)/5.0\n",
        "    \n",
        "  # Use this for single model\n",
        "#   start_logits_0 = torch.softmax(start_logits_0, dim=1).detach().cpu().numpy()\n",
        "#   end_logits_0 = torch.softmax(end_logits_0, dim=1).detach().cpu().numpy()\n",
        "#   start_prob = (start_logits_0)/1.0\n",
        "#   end_prob = (end_logits_0)/1.0\n",
        "\n",
        "\n",
        "  jac_= []\n",
        "  \n",
        "  for px, s in enumerate(text):\n",
        "    start_idx = np.argmax(start_prob[px,:])\n",
        "    end_idx = np.argmax(end_prob[px,:])\n",
        "\n",
        "    js, exp = evaluate_jaccard(s, expansion[px], acronym[px], offset[px], start_idx, end_idx)\n",
        "    jac_.append(js)\n",
        "    pred_expansion_.append(exp)\n",
        "\n",
        "  \n",
        "  jac.update(np.mean(jac_), len(jac_))\n",
        "  \n",
        "  tk0.set_postfix(jaccard=jac.avg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TITF8yiEddS8"
      },
      "source": [
        "test['pred_expansion'] = pred_expansion_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0cE8Kkod5Nx"
      },
      "source": [
        "predictions = []\n",
        "for i, r in test.iterrows():\n",
        "  d = {'id': r['id'], 'prediction': r['pred_expansion']}\n",
        "  predictions.append(d)\n",
        "\n",
        "with open(os.path.join('.', 'pred.json'), 'w') as f:\n",
        "  json.dump(predictions, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2Kkz0-bd6-m"
      },
      "source": [
        "test.to_csv('test_preds.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uspic4iud9No"
      },
      "source": [
        "predicti = []\n",
        "for i, r in test.iterrows():\n",
        "  d = {'sentence': r['text'], 'acronym': r['acronym_'], 'label': r['pred_expansion'], 'ID':str(r['id'])}\n",
        "  predicti.append(d)\n",
        "    \n",
        "with open(os.path.join('.', 'output.json'), 'w') as f:\n",
        "  json.dump(predicti, f)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}